{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "import math\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 节省数据内存操作\n",
    "- 读取用户点击日志、文章特征、文章文本emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 节省数据内存\n",
    "article_dtypes = {\n",
    "    \"article_id\": \"int32\",\n",
    "    \"category_id\": \"int16\",\n",
    "    \"created_at_ts\": \"int64\",\n",
    "    \"words_count\": \"int16\"}\n",
    "\n",
    "click_log_dtypes = {\n",
    "    \"user_id\": \"int32\",\n",
    "    \"click_article_id\": \"int32\",\n",
    "    \"click_timestamp\": \"int64\",\n",
    "    \"click_environment\": \"int8\",\n",
    "    \"click_deviceGroup\": \"int8\",\n",
    "    \"click_os\": \"int8\",\n",
    "    \"click_country\": \"int8\",\n",
    "    \"click_region\": \"int8\",\n",
    "    \"click_referrer_type\": \"int8\"}\n",
    "\n",
    "def get_train_click_df(data_save_path, name='train_click_log.csv'):\n",
    "    \"\"\"获取训练样本\"\"\"\n",
    "    train_df = pd.read_csv(data_save_path + name, dtype=click_log_dtypes)\n",
    "    return train_df\n",
    "\n",
    "def get_test_click_df(data_save_path, name='testA_click_log.csv'):\n",
    "    \"\"\"获取测试样本\"\"\"\n",
    "    test_df = pd.read_csv(data_save_path + name, dtype=click_log_dtypes)\n",
    "    return test_df\n",
    "\n",
    "def get_item_info_df(data_save_path, name='articles.csv'):\n",
    "    \"\"\"获取文章特征\"\"\"\n",
    "    item_info_df = pd.read_csv(data_save_path + name,  dtype=article_dtypes)\n",
    "    item_info_df = item_info_df.rename(columns={'article_id': 'click_article_id'})\n",
    "    return item_info_df\n",
    "\n",
    "def get_item_emb_df(data_save_path, name='articles_emb.csv'):\n",
    "    \"\"\"获取文章文本emb\"\"\"\n",
    "    emb_df = pd.read_csv(data_save_path + name)\n",
    "    emb_df = emb_df.rename(columns={'article_id': 'click_article_id'})\n",
    "    return emb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/\"\n",
    "offline = False # 离线测试，划分训练集与验证集\n",
    "if offline:\n",
    "    trn_df = get_train_click_df(data_path, \"offline_trn_df.csv\")\n",
    "    val_df = get_test_click_df(data_path, \"offline_val_df.csv\")\n",
    "else:\n",
    "    trn_df = get_train_click_df(data_path, \"train_click_log.csv\")\n",
    "    \n",
    "item_info_df = get_item_info_df(data_path)\n",
    "item_emb_df =  get_item_emb_df(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 召回策略1：热度召回\n",
    "- 普通方案：直接对新闻id做一个value_counts然后返回前5个文章（没有考虑文章的时效性）\n",
    "- 我的方案：考虑每个用户最后一次点击的时间，返回一个区间内的热门文章，可设置为超参数\n",
    "\n",
    "主要用于文本补充，当其他样本无法满足召回需求，则用热度召回补充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# 普通方案：获取近期点击最多的文章\n",
    "def get_item_topk_list(df, k):\n",
    "    topk_list = df['click_article_id'].value_counts().index[: k]\n",
    "    return topk_list\n",
    "\"\"\"\n",
    "\n",
    "# 改进方案：获取近期点击最多的文章，区间[-1.8*10^8, 1*10^5]\n",
    "def get_item_topk_click(click_df, k):\n",
    "    click_df = click_df[['user_id', 'click_article_id', 'created_at_ts']]\n",
    "    user_recall_items_dict = collections.defaultdict(dict)\n",
    "    for user in tqdm(click_df['user_id'].unique()):\n",
    "        user_last_click = user_item_time_dict[user][-1][1]\n",
    "        target_df = click_df.loc[(click_df['created_at_ts'] < user_last_click + 1 * (10 ** 5)) & (click_df['created_at_ts'] > user_last_click - 1.8 * (10 ** 8))]\n",
    "        \n",
    "        tmp_df = target_df['click_article_id'].value_counts()[: k] / len(target_df['click_article_id'])\n",
    "        user_recall_items_dict[user] = dict(tmp_df)\n",
    "    return user_recall_items_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 召回策略2：itemCF\n",
    "\n",
    "i2i_sim物品相似度矩阵计算：\n",
    "- **考虑文章的正向顺序点击和反向顺序点击**：如果是按照时间戳正向的话权重设为1，反向的话设一个小于1的超参数，因为考虑到网页间的图结构，经常是点完一个后下面有相关推荐，用户很可能点击下一个，反之则基本不会。\n",
    "- **考虑文章的位置信息权重**：考虑到了随时间的信息衰减，如果两个文章点击的次序距离太远，做一个缩放，0.8**(np.abs(loc2 - loc1) – 1)，也设一个超参数。\n",
    "- **考虑文章的点击时间权重**：这个是用户对物品的一个行为特征，与位置信息权重的想法类似，用一个指数来进行衰减。\n",
    "- **考虑文章创建时间的点击权重**：物品本身的特征，与文章点击时间权重的处理方法相似。\n",
    "- 其他的还考虑了物品之间Embedding向量之间的相似度、物品是否属于同一个类别（这个类别是官方提供的，有400多种，我不是很明白），是的话乘1.1权重，不是就乘1.0、还有文章的字数，这些我尝试过加到相似性计算中，效果不是很好，后来就没加。\n",
    "\n",
    "itemCF推荐：\n",
    "- **文章创建时间和用户点击时间差**：强特，每一个用户推荐时，找到最后一次点击时间戳，然后设一个最早可能点击时间和最晚可能点击时间，如果相似文章的创建时间不在这一个区间内，就直接pass。\n",
    "- **文章创建时间差和相似文章对应文章在用户点击序列中的位置**：离最后一次点击越远的文章，其相似文章的权重就越低。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_item_time(df, user_col='user_id', item_col='click_article_id', time_col='click_timestamp'):\n",
    "    \"\"\"\n",
    "    根据点击时间获取用户的点击文章序列   \n",
    "    return: dict, {user1: [(item1, time1), (item2, time2)..]...}\n",
    "    \"\"\"\n",
    "    df = df[[user_col, item_col, time_col]]\n",
    "    df = df.sort_values('click_timestamp')\n",
    "    user_item_time_df = df.groupby(user_col)[item_col, time_col].apply(lambda group: make_item_time_tuple(group))\\\n",
    "                                                            .reset_index().rename(columns={0: 'item_time_list'})\n",
    "    user_item_time_dict = dict(zip(user_item_time_df['user_id'], user_item_time_df['item_time_list']))\n",
    "    \n",
    "    return user_item_time_dict\n",
    "\n",
    "\n",
    "def get_item_info_dict(df, item_col='click_article_id', time_col='created_at_ts', cate_col='category_id', word_cnt_col='words_count'):\n",
    "    \"\"\"\n",
    "    获取文章id对应的基本属性 \n",
    "    return: item_created_abs_time_dict, dict, {item: diff_timestamp}\n",
    "            item_created_time_dict, dict, {item: created_timestamp}\n",
    "            item_type_dict, dict, {item: cate}\n",
    "            item_words_dict, dict, {item: word_cnt}\n",
    "    \"\"\"\n",
    "    # 获取绝对时间字典\n",
    "    item_created_abs_time_dict = dict(zip(df[item_col], df[time_col]))\n",
    "    \n",
    "    # 获取归一化时间字典\n",
    "    max_min_scaler = lambda x : (x-np.min(x))/(np.max(x)-np.min(x))\n",
    "    df[time_col] = df[[time_col]].apply(max_min_scaler)\n",
    "    item_created_time_dict = dict(zip(df[item_col], df[time_col]))\n",
    "    \n",
    "    # 获取文章类型字典\n",
    "    item_type_dict = dict(zip(df[item_col], df[cate_col]))\n",
    "    \n",
    "    # 获取文章字数字典\n",
    "    item_words_dict = dict(zip(df[item_col], df[word_cnt_col]))\n",
    "    \n",
    "    return item_created_abs_time_dict, item_created_time_dict, item_type_dict, item_words_dict\n",
    "\n",
    "def make_item_time_tuple(group_df, user_col='user_id', item_col='click_article_id', time_col='click_timestamp'):\n",
    "    item_time_tuple = list(zip(group_df[item_col], group_df[time_col]))\n",
    "    return item_time_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def itemcf_sim(df, item_created_time_dict, i2i_sim_save_path='./sim_matrix/'):\n",
    "    \"\"\"\n",
    "    文章与文章之间的相似性矩阵计算\n",
    "    return : i2i_sim, dict, {item1: {item2: weight2, item3: weight3, ...}}\n",
    "    \"\"\"\n",
    "    user_item_time_dict = get_user_item_time(df)\n",
    "    \n",
    "    # 计算相似度\n",
    "    i2i_sim = {}\n",
    "    item_cnt = collections.defaultdict(int)\n",
    "    for user, item_time_list in tqdm(user_item_time_dict.items()):\n",
    "        for loc1, (i, i_click_time) in enumerate(item_time_list):\n",
    "            item_cnt[i] += 1\n",
    "            i2i_sim.setdefault(i, {})\n",
    "            for loc2, (j, j_click_time) in enumerate(item_time_list):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                    \n",
    "                # 考虑文章的正向顺序点击和反向顺序点击    \n",
    "                loc_alpha = 1.0 if loc2 > loc1 else 0.7\n",
    "                # 位置信息权重\n",
    "                loc_weight = loc_alpha * (0.8 ** (np.abs(loc2 - loc1) - 1))\n",
    "                # 点击时间权重\n",
    "                click_time_weight = np.exp(0.8 ** np.abs(i_click_time - j_click_time))\n",
    "                # 两篇文章创建时间的权重\n",
    "                created_time_weight = np.exp(0.8 ** np.abs(item_created_time_dict[i] - item_created_time_dict[j]))\n",
    "                \n",
    "                i2i_sim[i].setdefault(j, 0)\n",
    "                # 计算文章之间的相似度\n",
    "                i2i_sim[i][j] += loc_weight * click_time_weight * created_time_weight / math.log(len(item_time_list) + 1)\n",
    "    \n",
    "    i2i_sim_ = i2i_sim.copy()\n",
    "    # 两篇文章的流行度权重，惩罚过于热门的物品，此处popular_weight设置为0.4时效果较好\n",
    "    popular_weight = 0.4\n",
    "    for i, related_items in i2i_sim.items():\n",
    "        for j, wij in related_items.items():\n",
    "            tmpMax, tmpMin = max(item_cnt[i], item_cnt[j]), min(item_cnt[i], item_cnt[j])\n",
    "            i2i_sim_[i][j] = wij / ((tmpMax ** popular_weight) * (tmpMin ** (1 - popular_weight)))\n",
    "    \n",
    "    # 将得到的相似性矩阵保存到本地\n",
    "    pickle.dump(i2i_sim_, open(i2i_sim_save_path + 'itemcf_i2i_sim.pkl', 'wb'))\n",
    "    \n",
    "    return i2i_sim_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_based_recommend(user_id, user_item_time_dict, i2i_sim, sim_item_topk, recall_item_num, topk_list, item_created_time_dict, item_created_abs_time_dict, emb_i2i_sim=None):\n",
    "    \"\"\"\n",
    "    基于文章协同过滤的召回\n",
    "    return: item_rank, list, [(item1, score1), (item2, score2)...]\n",
    "    \"\"\"\n",
    "    # 获取用户历史交互的文章\n",
    "    user_hist_items = user_item_time_dict[user_id]\n",
    "    user_hist_items_ = {item_id for item_id, _ in user_hist_items}\n",
    "    \n",
    "    item_rank = {}\n",
    "    for loc, (i, click_time) in enumerate(user_hist_items):\n",
    "        for j, wij in sorted(i2i_sim[i].items(), key=lambda x: x[1], reverse=True)[: sim_item_topk]: # 先选取每篇文章前sim_item_topk个相似文章\n",
    "\n",
    "            if j in user_hist_items_:\n",
    "                # 用户已经看过j文章了\n",
    "                continue\n",
    "            \n",
    "            # 文章生成及用户点击时间权重，设置一个区间，不在区间内的直接pass\n",
    "            if item_created_abs_time_dict[j] > user_item_time_dict[user_id][-1][1] + 1 * (10 ** 5) or item_created_abs_time_dict[j] < user_item_time_dict[user_id][-1][1] - 1.8 * (10 ** 8):\n",
    "                continue\n",
    "                \n",
    "            # 文章创建时间差权重\n",
    "            created_time_weight = np.exp(0.9 ** np.abs(item_created_time_dict[i] - item_created_time_dict[j]))\n",
    "            \n",
    "            # 相似文章和历史点击文章序列中历史文章所在的位置权重\n",
    "            loc_weight = (0.9 ** (len(user_hist_items) - loc))\n",
    "            \n",
    "            # embedding 相似性，效果不好，设置一个小权重，或者直接不用\n",
    "            content_weight = 1.0\n",
    "            if emb_i2i_sim:\n",
    "                if emb_i2i_sim.get(i, {}).get(j, None) is not None:\n",
    "                    content_weight += emb_i2i_sim[i][j]\n",
    "                if emb_i2i_sim.get(j, {}).get(i, None) is not None:\n",
    "                    content_weight += emb_i2i_sim[j][i]\n",
    "                content_weight = np.exp(1.0005 ** content_weight)\n",
    "            \n",
    "            item_rank.setdefault(j, 0)\n",
    "            item_rank[j] += content_weight * created_time_weight * loc_weight * wij\n",
    "            \n",
    "    # 不足10个，用热门商品补全\n",
    "    if len(item_rank) < recall_item_num:\n",
    "        for i, item in enumerate(topk_list):\n",
    "            if item in item_rank.items(): # 填充的item应该不在原来的列表中\n",
    "                continue\n",
    "            item_rank[item] = - i - 100 # 随便给个负数就行\n",
    "            if len(item_rank) == recall_item_num:\n",
    "                break\n",
    "    \n",
    "    item_rank = sorted(item_rank.items(), key=lambda x: x[1], reverse=True)[: recall_item_num]\n",
    "        \n",
    "    return item_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 召回策略3：swing\n",
    "\n",
    "其实一开始我是想用UserCF也计算一下的，因为新闻本身的兴趣点就比较分散，用户更倾向于追求新闻的及时性和热点性，UserCF就能比较好地把握这一点。但实际跑起来的时候内存爆了，没办法用，但还是想把用户彼此之间的信息融入到我的召回模型里。所以找到了阿里的协同过滤推荐算法swing，\n",
    "Swing的主要思想是基于图结构（u1,u2,i1），用户1和用户2都看过文章1和2，那说明1和2有关联，如果这种三元组数量越多，说明权重越大。\n",
    "算法：计算两个用户阅读文章的交集，权重分子与之间item相似，分母则对两个用户的交集长度做了一个补充，如果这两个用户交集长度很小，则说明他们的兴趣差异很大，但同时看过这篇文章，说明这两篇文章相似度很高。\n",
    "\n",
    "但还是爆内存了，这个图结构本质上是三阶特征，本身一二阶特征都够呛了，所以我考虑采样，方法是扔掉历史观看序列很长（大于8）的活跃用户样本，扔掉观看序列较小（小于3）的不活跃用户样本，剩下约63%的样本，然后进行计算。这个想法是我拍脑袋决定的，因为活跃用户和不活跃用户的参考价值都比较小，其实主要目的就是想至少算出来个数，来看看结果，结果还行，有0.18左右的得分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swing(df, user_col='user_id', item_col='click_article_id', time_col='click_timestamp', swing_sim_save_path='./sim_matrix/'):\n",
    "    \"\"\"\n",
    "    基于swing的召回\n",
    "    return: sim_item_corr, dict, {item1: {item2: weight2, item3: weight3, ...}}\n",
    "    \"\"\"\n",
    "    # item, (u1,t1), (u2, t2).....\n",
    "    item_user_df = df.sort_values(by=[item_col, time_col])\n",
    "    item_user_df = item_user_df.groupby(item_col).apply(lambda group: make_user_time_tuple(group, user_col, item_col, time_col)).reset_index().rename(\n",
    "        columns={0: 'user_id_time_list'})\n",
    "    \n",
    "    item_user_time_dict = dict(zip(item_user_df[item_col], item_user_df['user_id_time_list']))\n",
    "    \n",
    "    # ((u1, u2), i1, d12)\n",
    "    u_u_cnt = collections.defaultdict(list)\n",
    "    item_cnt = collections.defaultdict(int)\n",
    "    for item, user_time_list in tqdm(item_user_time_dict.items()):\n",
    "        for u, u_time in user_time_list:\n",
    "            item_cnt[item] += 1\n",
    "            for relate_u, relate_u_time in user_time_list:\n",
    "                if relate_u == u:\n",
    "                    continue\n",
    "                key = (u, relate_u) if u <= relate_u else (relate_u, u)\n",
    "                u_u_cnt[key].append((item, np.abs(u_time - relate_u_time)))\n",
    "                \n",
    "    # (i1,i2), sim\n",
    "    sim_item = {}\n",
    "    alpha = 5.0\n",
    "    for u_u, co_item_times in u_u_cnt.items():\n",
    "        num_co_items = len(co_item_times)\n",
    "        for i, i_time_diff in co_item_times:\n",
    "            sim_item.setdefault(i, {})\n",
    "            for j, j_time_diff in co_item_times:\n",
    "                if j == i:\n",
    "                    continue\n",
    "                weight = 1.0  # np.exp(-15000*(i_time_diff + j_time_diff))\n",
    "                sim_item[i][j] = sim_item[i].setdefault(j, 0.) + weight / (alpha + num_co_items)\n",
    "                \n",
    "    # norm by item count\n",
    "    sim_item_corr = sim_item.copy()\n",
    "    for i, related_items in sim_item.items():\n",
    "        for j, cij in related_items.items():\n",
    "            sim_item_corr[i][j] = cij / math.sqrt(item_cnt[i] * item_cnt[j])\n",
    "    \n",
    "    pickle.dump(sim_item_corr, open(swing_sim_save_path + 'swing_i2i_sim.pkl', 'wb'))\n",
    "    return sim_item_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 召回策略4：item2vec\n",
    "\n",
    "将用户的点击序列扔到w2v模型里训练，得到每个新闻的Embedding向量，然后根据用户历史点击的新闻选取最相似的新闻推荐。（item2vec是没有时间窗概念的，认为序列中任意两个物品都相关），但比赛里我还是用的gensim库里的Word2Vec训练器（size: 表示词向量的维度。window：决定了目标词会与多远距离的上下文产生关系。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item2vec(df, embed_size=64, emb_save_path='./embed/', split_char=' '):\n",
    "    \"\"\"\n",
    "    基于word2vec的召回\n",
    "    return: item_w2v_emb_dict, dict, {item1: embedding}\n",
    "    \"\"\"\n",
    "    df = df.sort_values('click_timestamp')\n",
    "    # 转换成字符串\n",
    "    click_df['click_article_id'] = click_df['click_article_id'].astype(str)\n",
    "    # 转换成句子的形式\n",
    "    docs = click_df.groupby(['user_id'])['click_article_id'].apply(lambda x: list(x)).reset_index()\n",
    "    docs = docs['click_article_id'].values.tolist()\n",
    "\n",
    "    logging.basicConfig(format='%(asctime)s:%(levelname)s:%(message)s', level=logging.INFO)\n",
    "\n",
    "    # window选择为5\n",
    "    w2v = Word2Vec(docs, size=embed_size, sg=1, window=5, seed=2020, workers=24, min_count=1, iter=1)\n",
    "    \n",
    "    # 保存成字典的形式\n",
    "    item_w2v_emb_dict = {k: w2v[k] for k in click_df['click_article_id']}\n",
    "    pickle.dump(item_w2v_emb_dict, open(emb_save_path + 'item_w2v_emb.pkl', 'wb'))\n",
    "    \n",
    "    return item_w2v_emb_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 离线测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取所有数据的历史点击和最后一次点击，制作离线训练集和测试集\n",
    "def get_hist_and_last_click(click_df):\n",
    "    click_df = click_df.sort_values(by=['user_id', 'click_timestamp'])\n",
    "    click_last_df = click_df.groupby('user_id').tail(1)\n",
    "    \n",
    "    # 如果用户只有一个点击，hist为空了，会导致训练的时候这个用户不可见，将数据放入到训练集\n",
    "    def hist_func(user_df):\n",
    "        if len(user_df) == 1:\n",
    "            return user_df\n",
    "        else:\n",
    "            return user_df[: -1]\n",
    "        \n",
    "    click_hist_df = click_df.groupby('user_id').apply(hist_func).reset_index(drop=True)\n",
    "    return click_hist_df, click_last_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 依次评估召回的前10, 20, 30, 40, 50个文章中的击中率\n",
    "def metrics_recall(user_recall_items_dict, trn_last_click_df, topk=5):\n",
    "    last_click_item_dict = dict(zip(trn_last_click_df['user_id'], trn_last_click_df['click_article_id']))\n",
    "    user_num = len(user_recall_items_dict)\n",
    "    for k in range(10, 51, 10):\n",
    "        hit_num = 0\n",
    "        for user, item_list in user_recall_items_dict.items():\n",
    "            # 获取前k个召回的结果\n",
    "            tmp_recall_items = [x[0] for x in user_recall_items_dict[user][: k]]\n",
    "            if last_click_item_dict[user] in set(tmp_recall_items):\n",
    "                hit_num += 1\n",
    "        \n",
    "        hit_rate = round(hit_num * 1.0 / user_num, 5)\n",
    "        print(' topk: ', k, ' : ', 'hit_num: ', hit_num, 'hit_rate: ', hit_rate, 'user_num : ', user_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_time_dict = get_user_item_time(trn_df)\n",
    "item_created_abs_time_dict, item_created_time_dict, item_type_dict, item_words_dict = get_item_info_dict(item_info_df)\n",
    "i2i_sim_save_path = \"./sim_matrix/offline_itemcf_i2i_sim.pkl\"\n",
    "i2i_sim = itemcf_sim(total_df, item_created_time_dict, i2i_sim_save_path)\n",
    "i2i_sim = pickle.load(open(i2i_sim_save_path, 'rb'))\n",
    "topk_list = get_item_topk_list(total_df, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_item_topk = 100 # 选择与当前文章最相似的前sim_item_topk篇文章\n",
    "recall_item_num = 50 # 最终召回sim_item_topk篇文章\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "user_recall_items_dict = multiprocessing.Manager().dict() # 多进程字典\n",
    "\n",
    "\n",
    "for user in tqdm(trn_df['user_id'].unique()):\n",
    "    user_recall_items_dict[user] = item_based_recommend(user, user_item_time_dict, \\\n",
    "                                                        i2i_sim, sim_item_topk, recall_item_num, \\\n",
    "                                                        topk_list, item_created_time_dict,item_created_abs_time_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df = pd.read_csv(\"../data/sample_submit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将字典的形式转换成df\n",
    "user_item_score_list = []\n",
    "\n",
    "for user, items in tqdm(user_recall_items_dict.items()):\n",
    "    for item, score in items:\n",
    "        user_item_score_list.append([user, item, score])\n",
    "\n",
    "recall_df = pd.DataFrame(user_item_score_list, columns=['user_id', 'click_article_id', 'pred_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成提交文件\n",
    "def submit(recall_df, topk=5, model_name=None):\n",
    "    recall_df = recall_df.sort_values(by=['user_id', 'pred_score'])\n",
    "    recall_df['rank'] = recall_df.groupby(['user_id'])['pred_score'].rank(ascending=False, method='first')\n",
    "    \n",
    "    # 判断是不是每个用户都有5篇文章及以上\n",
    "    tmp = recall_df.groupby('user_id').apply(lambda x: x['rank'].max())\n",
    "    assert tmp.min() >= topk\n",
    "    \n",
    "    del recall_df['pred_score']\n",
    "    submit = recall_df[recall_df['rank'] <= topk].set_index(['user_id', 'rank']).unstack(-1).reset_index()\n",
    "    \n",
    "    submit.columns = [int(col) if isinstance(col, int) else col for col in submit.columns.droplevel(0)]\n",
    "    # 按照提交格式定义列名\n",
    "    submit = submit.rename(columns={'': 'user_id', 1: 'article_1', 2: 'article_2', \n",
    "                                                  3: 'article_3', 4: 'article_4', 5: 'article_5'})\n",
    "    \n",
    "    save_name = './submit/' + datetime.today().strftime('%m-%d') + '.csv'\n",
    "    submit.to_csv(save_name, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit(recall_df, topk=5, model_name=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
